{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021e401c",
   "metadata": {},
   "source": [
    "# IR_InvertedMatrix_Workshop\n",
    "\n",
    "**Course:** Machine Learning Programming  \n",
    "**Team Number:** 5\n",
    "\n",
    "**Team Members:**  \n",
    "- Mandeep Singh (ID: 8989367)  \n",
    "- Kumari Nikitha Singh (ID: 9053016)  \n",
    "- Krishna (ID: 905861)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## üîç Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code‚Äîjust like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data ‚Äî such as AI agents.\n",
    "\n",
    "### üë• Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## üîß Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß† Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## üß© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* ‚Äì Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* ‚Äì Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* ‚Äì Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* ‚Äì Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## üìÑ Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### üó£ Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### üîß Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498bba55",
   "metadata": {},
   "source": [
    "# Step 1: Document Collection\n",
    "\n",
    "\n",
    "**Dataset  :**  \n",
    "We use the `20 Newsgroups` dataset provided by Scikit-learn‚Äôs `datasets` module.  \n",
    "This dataset includes thousands of newsgroup posts covering diverse topics and is widely used in Information Retrieval and Natural Language Processing research.\n",
    "\n",
    "By selecting a subset of 50 documents, we keep the pipeline practical for a classroom session while maintaining enough vocabulary diversity for realistic testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2823235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load training portion, removing headers, footers, and quotes for raw text only\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "# Extract first 50 documents for demonstration\n",
    "documents = newsgroups.data[:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7573c13",
   "metadata": {},
   "source": [
    "## Simple Document Viewer\n",
    "\n",
    "This helper function lets us check any document by its ID.  \n",
    "It is useful for verifying query results and reading the raw text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5644bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Document ID: 6 ---\n",
      "\n",
      "There were a few people who responded to my request for info on\n",
      "treatment for astrocytomas through email, whom I couldn't thank\n",
      "directly because of mail-bouncing probs (Sean, Debra, and Sharon).  So\n",
      "I thought I'd publicly thank everyone.\n",
      "\n",
      "Thanks! \n",
      "\n",
      "(I'm sure glad I accidentally hit \"rn\" instead of \"rm\" when I was\n",
      "trying to delete a file last September. \"Hmmm... 'News?' What's\n",
      "this?\"....)\n"
     ]
    }
   ],
   "source": [
    "# Simple document viewer to inspect a specific document by index\n",
    "\n",
    "def view_document(documents, doc_id):\n",
    "    \"\"\"\n",
    "    Prints the full text of a document by index.\n",
    "    Args:\n",
    "        documents (list): List of documents.\n",
    "        doc_id (int): Index of the document to view.\n",
    "    \"\"\"\n",
    "    if 0 <= doc_id < len(documents):\n",
    "        print(f\"--- Document ID: {doc_id} ---\\n\")\n",
    "        print(documents[doc_id])\n",
    "    else:\n",
    "        print(f\"Invalid doc_id: {doc_id}. Must be between 0 and {len(documents)-1}.\")\n",
    "\n",
    "# Example usage:\n",
    "# Replace 0 with any valid doc_id\n",
    "view_document(documents, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb45f5b",
   "metadata": {},
   "source": [
    "## Talking Point\n",
    "\n",
    "Using the Scikit-learn `20 Newsgroups` dataset allows us to work with realistic, naturally written text.  \n",
    "This replicates the conditions faced by search engines or NLP systems, which rarely deal with perfectly clean data.\n",
    "\n",
    "This step ensures our pipeline will be tested with authentic language structure, topic variety, and vocabulary spread.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a809ab",
   "metadata": {},
   "source": [
    "## Vocabulary Size Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a27e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens in corpus: 2544\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "all_tokens = []\n",
    "for doc in documents:\n",
    "    all_tokens.extend(basic_tokenize(doc))\n",
    "\n",
    "unique_tokens = set(all_tokens)\n",
    "print(f\"Total unique tokens in corpus: {len(unique_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48c319",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Checking the unique token count confirms that our dataset meets the target vocabulary size.  \n",
    "A realistic vocabulary ensures the inverted index is tested under conditions similar to real-world Information Retrieval tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d971ade",
   "metadata": {},
   "source": [
    "# Step 2: Tokenizer\n",
    "\n",
    "In this step, we transform raw text into a stream of tokens (words).  \n",
    "Tokenization is the foundation of any text processing pipeline, as it defines how the text is split and prepared for further normalization and indexing.\n",
    "\n",
    "**Approach:**  \n",
    "We implement a simple regular-expression-based tokenizer that:\n",
    "- Converts all text to lowercase to ensure consistent matching.\n",
    "- Removes punctuation and splits text into alphanumeric word tokens.\n",
    "- Filters out any stray characters or empty strings.\n",
    "\n",
    "This step ensures that our inverted index will have a clean, predictable vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: ['well', 'folks', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after', 'starting', 'life', 'as', 'a', '512k', 'way', 'back']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes input text by:\n",
    "    1. Lowercasing.\n",
    "    2. Extracting word tokens using regex.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Example: Tokenize the first document\n",
    "sample_tokens = tokenize(documents[2])\n",
    "print(\"Sample tokens:\", sample_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c4677",
   "metadata": {},
   "source": [
    "## Talking Point\n",
    "\n",
    "This basic tokenizer uses a word-boundary regular expression (`\\b\\w+\\b`) to identify word-like units.  \n",
    "Lowercasing standardizes word forms so that, for example, \"Machine\" and \"machine\" are treated as the same token.\n",
    "\n",
    "The output list provides the base input for later steps, such as stop word removal and stemming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9183450",
   "metadata": {},
   "source": [
    "## Tokenizer Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db60b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive split: ['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day.', 'it', 'was']\n",
      "Regex tokenizer: ['well', 'folks', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after', 'starting', 'life', 'as', 'a', '512k', 'way', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Compare with a naive split for demonstration\n",
    "def naive_split(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "split_tokens = naive_split(documents[0])\n",
    "\n",
    "print(\"Naive split:\", split_tokens[:20])\n",
    "print(\"Regex tokenizer:\", sample_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858aff9e",
   "metadata": {},
   "source": [
    "## Note on Tokenizer Choice\n",
    "\n",
    "A naive split on whitespace often leaves punctuation attached to words (e.g., \"learning.\").  \n",
    "A regex tokenizer handles punctuation more reliably, producing cleaner tokens and improving index quality.\n",
    "\n",
    "A robust tokenizer is critical for downstream tasks, as errors here propagate through the entire Information Retrieval pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d04ce5",
   "metadata": {},
   "source": [
    "# Step 3: Normalization Pipeline\n",
    "\n",
    "After tokenization, we normalize tokens to reduce redundancy and unify word forms.  \n",
    "Normalization improves search accuracy by ensuring that different forms of the same word map to a single root representation.\n",
    "\n",
    "**Key tasks in this step:**\n",
    "- Remove common stop words (e.g., \"the\", \"is\") that add noise but little meaning.\n",
    "- Apply stemming or lemmatization to reduce words to their base form.\n",
    "\n",
    "This ensures our inverted index groups related terms and reduces vocabulary fragmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506385e",
   "metadata": {},
   "source": [
    "### NLTK setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kittu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kittu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words, stemmer, and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3997225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(tokens, method='stem'):\n",
    "    \"\"\"\n",
    "    Normalizes a list of tokens by:\n",
    "    1. Removing stop words.\n",
    "    2. Applying stemming or lemmatization.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): List of word tokens.\n",
    "        method (str): 'stem' or 'lemmatize'.\n",
    "\n",
    "    Returns:\n",
    "        list: Normalized tokens.\n",
    "    \"\"\"\n",
    "    if method == 'stem':\n",
    "        normalized = [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "    elif method == 'lemmatize':\n",
    "        normalized = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'stem' or 'lemmatize'.\")\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6be16063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: ['well', 'folk', 'mac', 'plu', 'final', 'gave', 'ghost', 'weekend', 'start', 'life', '512k', 'way', 'back', '1985', 'sooo', 'market', 'new', 'machin', 'bit', 'sooner']\n",
      "Lemmatized: ['well', 'folk', 'mac', 'plus', 'finally', 'gave', 'ghost', 'weekend', 'starting', 'life', '512k', 'way', 'back', '1985', 'sooo', 'market', 'new', 'machine', 'bit', 'sooner']\n"
     ]
    }
   ],
   "source": [
    "# Example: Normalize sample tokens\n",
    "stemmed_tokens = normalize_tokens(sample_tokens, method='stem')\n",
    "lemmatized_tokens = normalize_tokens(sample_tokens, method='lemmatize')\n",
    "\n",
    "print(\"Stemmed:\", stemmed_tokens[:20])\n",
    "print(\"Lemmatized:\", lemmatized_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c145686",
   "metadata": {},
   "source": [
    "## Talking Point\n",
    "\n",
    "Stop word removal filters out common words that do not significantly contribute to information retrieval tasks.\n",
    "\n",
    "Stemming reduces words to their root forms using heuristic rules.  \n",
    "For example, \"running\" becomes \"run\".\n",
    "\n",
    "Lemmatization uses vocabulary and grammar rules to map words to their base or dictionary form.  \n",
    "For example, \"better\" maps to \"good\".\n",
    "\n",
    "Both methods simplify the token list, but lemmatization often provides more accurate root forms for phrase queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814049ca",
   "metadata": {},
   "source": [
    "## Note on Method Choice\n",
    "\n",
    "Stemming is faster and sufficient for many search tasks where exact word forms are not critical.\n",
    "\n",
    "Lemmatization is more accurate but requires linguistic resources.  \n",
    "It is better suited for applications where grammatical correctness matters, such as document summarization or answer generation.\n",
    "\n",
    "In practice, large-scale search engines often use custom pipelines that combine both approaches, depending on language and domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c38cd3",
   "metadata": {},
   "source": [
    "# Step 4: Inverted Index with Positional Indexing\n",
    "\n",
    "In this step, we build the inverted index ‚Äî a core data structure for fast document retrieval.\n",
    "\n",
    "An inverted index maps each unique term to the list of document IDs where it appears.  \n",
    "To support phrase queries, we also store the positions of each term within each document.  \n",
    "This allows the system to check if words appear in the correct sequence.\n",
    "\n",
    "A position-aware inverted index is a simplified version of the data structures used in production search engines like Lucene or Elasticsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf09abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_positional_index(documents, normalization_method='stem'):\n",
    "    \"\"\"\n",
    "    Builds a positional inverted index for a list of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of text documents.\n",
    "        normalization_method (str): 'stem' or 'lemmatize'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Nested dictionary mapping term -> doc_id -> positions.\n",
    "    \"\"\"\n",
    "    index = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = tokenize(text)\n",
    "        norm_tokens = normalize_tokens(tokens, method=normalization_method)\n",
    "        for position, token in enumerate(norm_tokens):\n",
    "            index[token][doc_id].append(position)\n",
    "\n",
    "    return index\n",
    "\n",
    "# Build the index using stemming\n",
    "positional_index = build_positional_index(documents, normalization_method='stem')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322e27e",
   "metadata": {},
   "source": [
    "## Preview first few terms in a readable format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28a3b46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Term: 'wonder'\n",
      "  Doc ID: 0 ‚Üí Positions: [ 0 ]\n",
      "  Doc ID: 2 ‚Üí Positions: [ 51 ]\n",
      "\n",
      "Term: 'anyon'\n",
      "  Doc ID: 0 ‚Üí Positions: [ 1 | 28 ]\n",
      "  Doc ID: 16 ‚Üí Positions: [ 113 | 116 ]\n",
      "  Doc ID: 18 ‚Üí Positions: [ 24 ]\n",
      "  Doc ID: 25 ‚Üí Positions: [ 16 ]\n",
      "  Doc ID: 40 ‚Üí Positions: [ 45 ]\n",
      "\n",
      "Term: 'could'\n",
      "  Doc ID: 0 ‚Üí Positions: [ 2 ]\n",
      "  Doc ID: 2 ‚Üí Positions: [ 69 | 90 | 135 ]\n",
      "  Doc ID: 11 ‚Üí Positions: [ 24 ]\n",
      "  Doc ID: 17 ‚Üí Positions: [ 832 ]\n",
      "  Doc ID: 37 ‚Üí Positions: [ 134 | 165 | 178 | 213 ]\n"
     ]
    }
   ],
   "source": [
    "# Preview first few terms in a readable format\n",
    "for term in list(positional_index.keys())[:3]:\n",
    "    print(f\"\\nTerm: '{term}'\")\n",
    "    for doc_id, positions in positional_index[term].items():\n",
    "        pretty_pos = \" | \".join(str(p) for p in positions)\n",
    "        print(f\"  Doc ID: {doc_id} ‚Üí Positions: [ {pretty_pos} ]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea90897",
   "metadata": {},
   "source": [
    "## Talking Point\n",
    "\n",
    "The positional inverted index maps each normalized term to:\n",
    "- The IDs of documents that contain the term.\n",
    "- The exact positions where the term appears within each document.\n",
    "\n",
    "This extra position information allows the system to answer not only \"Does this term exist in this document?\" but also \"Do these terms appear next to each other in the correct order?\"\n",
    "\n",
    "This makes phrase queries possible and efficient, which is essential for building practical search functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a6e7a",
   "metadata": {},
   "source": [
    "# Step 5: Test ‚Äî Phrase Queries\n",
    "\n",
    "To verify that our positional inverted index works as expected, we run two phrase queries.\n",
    "\n",
    "A phrase query checks whether the exact sequence of words appears in the document.  \n",
    "Unlike a basic keyword search, this guarantees that the words are in the correct order and next to each other.\n",
    "\n",
    "We test with two phrases:\n",
    "- A general technical term that likely exists in the dataset.\n",
    "- A second phrase to confirm that the position logic works for different term combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed41fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5 ‚Äî Phrase / AND query with clear per-word doc list ===\n",
    "\n",
    "def term_query(index, query_terms, normalization_method='stem'):\n",
    "    \"\"\"\n",
    "    Finds docs where ALL words appear anywhere.\n",
    "    \"\"\"\n",
    "    norm_terms = normalize_tokens(query_terms, method=normalization_method)\n",
    "    postings = [set(index.get(term, {}).keys()) for term in norm_terms if term in index]\n",
    "    if not postings:\n",
    "        return [], norm_terms\n",
    "    return list(set.intersection(*postings)), norm_terms\n",
    "\n",
    "\n",
    "def run_term_query_and_show_positions(index, documents, phrase, normalization_method='stem', preview_len=250):\n",
    "    \"\"\"\n",
    "    Runs AND term query. Shows docs where ALL words have positions.\n",
    "    Also prints docs for each single word.\n",
    "    \"\"\"\n",
    "    line = \"-\" * 60\n",
    "    query_terms = phrase.strip().lower().split()\n",
    "    term_docs, norm_terms = term_query(index, query_terms, normalization_method=normalization_method)\n",
    "\n",
    "    print(f\"\\n{line}\")\n",
    "    print(f\"üîç QUERY: '{phrase}'\")\n",
    "\n",
    "    if not term_docs:\n",
    "        print(f\"‚ùå No matching documents found.\")\n",
    "        print(f\"{line}\")\n",
    "        return\n",
    "\n",
    "    for doc_id in term_docs:\n",
    "        missing = []\n",
    "        word_positions = {}\n",
    "\n",
    "        for term in norm_terms:\n",
    "            pos_list = index.get(term, {}).get(doc_id, [])\n",
    "            if not pos_list:\n",
    "                missing.append(term)\n",
    "            else:\n",
    "                word_positions[term] = pos_list\n",
    "\n",
    "        if missing:\n",
    "            print(f\"üö´ SKIPPED ‚Üí Document ID: {doc_id} | Missing: {', '.join(missing)}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚úÖ MATCH ‚Üí Document ID: {doc_id}\")\n",
    "        for term in norm_terms:\n",
    "            pretty_pos = \" | \".join(str(p) for p in word_positions[term])\n",
    "            print(f\"   ‚Ä¢ word: '{term}' ‚Üí positions: [ {pretty_pos} ]\")\n",
    "\n",
    "        print(\"\\n   Excerpt:\")\n",
    "        print(f\"   \\\"{documents[doc_id][:preview_len]}...\\\"\")\n",
    "        print(f\"{line}\")\n",
    "\n",
    "    # === NEW: Clear per-word doc list ===\n",
    "    print(\"\\nDocs for individual words (anywhere):\")\n",
    "    for term in norm_terms:\n",
    "        docs_for_term = sorted(index.get(term, {}).keys())\n",
    "        print(f\"   ‚Ä¢ '{term}': {docs_for_term}\")\n",
    "    print(f\"{line}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3c97b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "üîç QUERY: 'email people'\n",
      "‚úÖ MATCH ‚Üí Document ID: 2\n",
      "   ‚Ä¢ word: 'email' ‚Üí positions: [ 136 ]\n",
      "   ‚Ä¢ word: 'peopl' ‚Üí positions: [ 93 ]\n",
      "\n",
      "   Excerpt:\n",
      "   \"well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 ...\"\n",
      "------------------------------------------------------------\n",
      "‚úÖ MATCH ‚Üí Document ID: 6\n",
      "   ‚Ä¢ word: 'email' ‚Üí positions: [ 6 ]\n",
      "   ‚Ä¢ word: 'peopl' ‚Üí positions: [ 0 ]\n",
      "\n",
      "   Excerpt:\n",
      "   \"There were a few people who responded to my request for info on\n",
      "treatment for astrocytomas through email, whom I couldn't thank\n",
      "directly because of mail-bouncing probs (Sean, Debra, and Sharon).  So\n",
      "I thought I'd publicly thank everyone.\n",
      "\n",
      "Thanks! \n",
      "\n",
      "(...\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Docs for individual words (anywhere):\n",
      "   ‚Ä¢ 'email': [2, 6, 10, 17, 25]\n",
      "   ‚Ä¢ 'peopl': [2, 6, 15, 28, 29, 30, 37, 39, 40, 44, 48]\n",
      "------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "üîç QUERY: 'space shuttle'\n",
      "‚úÖ MATCH ‚Üí Document ID: 13\n",
      "   ‚Ä¢ word: 'space' ‚Üí positions: [ 44 | 50 | 234 ]\n",
      "   ‚Ä¢ word: 'shuttl' ‚Üí positions: [ 45 | 51 | 131 | 148 | 191 ]\n",
      "\n",
      "   Excerpt:\n",
      "   \"\n",
      "   {Description of \"External Tank\" option for SSF redesign deleted}\n",
      "\n",
      "\n",
      "Yo Ken, let's keep on-top of things! Both the \"External Tank\" and\n",
      "\"Wingless Orbiter\" options have been deleted from the SSF redesign\n",
      "options list. Today's (4/23) edition of the Ne...\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Docs for individual words (anywhere):\n",
      "   ‚Ä¢ 'space': [13, 49]\n",
      "   ‚Ä¢ 'shuttl': [13]\n",
      "------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "üîç QUERY: 'computer graphics'\n",
      "‚ùå No matching documents found.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Example queries ===\n",
    "\n",
    "query_1 = \"email people\"\n",
    "query_2 = \"space shuttle\"\n",
    "query_3 = \"computer graphics\"\n",
    "\n",
    "run_term_query_and_show_positions(positional_index, documents, query_1)\n",
    "run_term_query_and_show_positions(positional_index, documents, query_2)\n",
    "run_term_query_and_show_positions(positional_index, documents, query_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c9a7a",
   "metadata": {},
   "source": [
    "## Step 5 Queries and Results\n",
    "\n",
    "Below are example queries to test the inverted index.\n",
    "Each shows document IDs, word positions, and a text excerpt for context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3e5d4",
   "metadata": {},
   "source": [
    "## Note on Stop Words\n",
    "\n",
    "In this project, common words like *‚Äúis‚Äù* and *‚Äúthe‚Äù* are treated as stop words.  \n",
    "During normalization, they are removed to keep the index smaller and more relevant.  \n",
    "So, any query using only stop words (e.g., `\"is the\"`) will not return results, because those words are not indexed.\n",
    "\n",
    "This is standard in real IR pipelines.  \n",
    "If needed for testing, you could keep stop words by disabling stop word removal when building the index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e39b3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, we implemented a simple but realistic inverted index with positional information.  \n",
    "By combining document collection, tokenization, normalization, and indexing, we showed how search systems can find words, phrases, and their exact locations inside real text.  \n",
    "\n",
    "Our tests confirm that the index correctly returns matching documents, word positions, and excerpts for inspection.  \n",
    "This pipeline forms the foundation for more advanced Information Retrieval tasks such as ranking, scoring, and large-scale search engines.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
